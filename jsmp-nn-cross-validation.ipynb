{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install ../input/python-datatable/datatable-0.11.0-cp37-cp37m-manylinux2010_x86_64.whl > /dev/null 2>&1\nimport datatable as dt\n\n# system\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nimport keras.backend as K\nimport tensorflow as tf\nfrom tensorflow import keras\n\nimport numpy as np\nfrom scipy.stats import norm\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom sklearn.metrics import confusion_matrix,accuracy_score,precision_score,recall_score\nfrom sklearn.utils import class_weight\n\nfrom tqdm import tqdm\n\nimport janestreet","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Options\nDisplay = False\nInf = False\nTRAINING = True\n\n# seed set\nSEED = 2021\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reading"},{"metadata":{"trusted":true},"cell_type":"code","source":"folder_path = '../input/jane-street-market-prediction/'\nsave_path = '../input/jane-autoencorder/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading csv\nprint('Reading train.csv')\ntrain = dt.fread(folder_path + 'train.csv').to_pandas()\n\nprint('Finish Reading')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pre-processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pre-process\nprint('Pre-process start')\ntrain = train.astype({c: np.float32 for c in train.select_dtypes(include='float64').columns}) #limit memory use\ntrain = train[train['weight'] > 0.0]\n#train = train[train['date'] > 99].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Enginiering"},{"metadata":{"trusted":true},"cell_type":"code","source":"# action = 0とするresp(=featureのリターン)を学習させるターゲットとする\ntrain['action'] = (train['resp']> 0.0).astype('int8')\ntrain['action_1'] = (train['resp_1']> 0.0).astype('int8')\ntrain['action_2'] = (train['resp_2']> 0.0).astype('int8')\ntrain['action_3'] = (train['resp_3']> 0.0).astype('int8')\ntrain['action_4'] = (train['resp_4']> 0.0).astype('int8')\n\n# features\n\nselect_feature = False # True\nif select_feature:\n    # lgbm_feature_importance_top...\n    numbers = [18]\n\n    features = [c for c in train.columns if \"feature\" in c if int(c.split('_')[-1]) in numbers]\nelse:\n    features = [c for c in train.columns if \"feature\" in c]\n\ntarget = 'action'\nmulti_target = ['action','action_1','action_2','action_3','action_4']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f_mean = np.mean(train[features[1:]].values,axis=0)\n\ntrain.fillna(train.mean(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train[features] = (train[features]>0.0).astype('int8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display\nif Display:\n    print('desccribe')\n    display(train.describe())\n    print(train.dtypes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Metric"},{"metadata":{"trusted":true},"cell_type":"code","source":"def utility_score_numba(date, weight, resp, action):\n    Pi = np.bincount(date, weight * resp * action)\n    t = np.sum(Pi) / np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 / len(Pi))\n    u = min(max(t, 0), 6) * np.sum(Pi)\n    return u\n\ndef jane_utility(data, action_column=\"action\"):\n    return utility_score_numba(data[\"date\"].values, data[\"weight\"].values, data[\"resp\"].values, data[action_column].values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class WeightedBinaryCrossEntropy(keras.losses.Loss):\n    \"\"\"\n    Args:\n      pos_weight: Scalar to affect the positive labels of the loss function.\n      weight: Scalar to affect the entirety of the loss function.\n      from_logits: Whether to compute loss form logits or the probability.\n      reduction: Type of tf.keras.losses.Reduction to apply to loss.\n      name: Name of the loss function.\n    \"\"\"\n    def __init__(self, pos_weight, weight, from_logits=False,\n                 reduction=keras.losses.Reduction.AUTO,\n                 name='weighted_binary_crossentropy'):\n        super(WeightedBinaryCrossEntropy, self).__init__(reduction=reduction,\n                                                         name=name)\n        self.pos_weight = pos_weight\n        self.weight = weight\n        self.from_logits = from_logits\n \n    def call(self, y_true, y_pred):\n        if not self.from_logits:\n            # Manually calculate the weighted cross entropy.\n            # Formula is qz * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n            # where z are labels, x is logits, and q is the weight.\n            # Since the values passed are from sigmoid (assuming in this case)\n            # sigmoid(x) will be replaced by y_pred\n \n            # qz * -log(sigmoid(x)) 1e-6 is added as an epsilon to stop passing a zero into the log\n            x_1 = y_true * self.pos_weight * -tf.math.log(y_pred + 1e-6)\n \n            # (1 - z) * -log(1 - sigmoid(x)). Epsilon is added to prevent passing a zero into the log\n            x_2 = (1 - y_true) * -tf.math.log(1 - y_pred + 1e-6)\n \n            return tf.add(x_1, x_2) * self.weight\n \n        # Use built in function\n        return tf.nn.weighted_cross_entropy_with_logits(y_true, y_pred, self.pos_weight) * self.weight","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_mlp(input_dim, output_dim, label_smoothing, learning_rate):\n\n    inputs = Input(input_dim)\n    \n    x = BatchNormalization()(inputs)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    \n    x = tf.keras.layers.Dense(160)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n    x = tf.keras.layers.Dropout(0.2)(x)\n\n    x = tf.keras.layers.Dense(160)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n    x = tf.keras.layers.Dropout(0.2)(x)\n\n    out = Dense(output_dim, activation='sigmoid')(x)\n   \n    model = Model(inputs=inputs, outputs=out)\n    \n    model.compile(\n        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate),\n        loss= WeightedBinaryCrossEntropy(0.9, 1.0),\n        #loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing),\n        #loss = tf.keras.losses.BinaryCrossentropy(),\n        metrics = tf.keras.metrics.AUC(name=\"AUC\"),\n    )\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Traning,Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 1000\nbatch_size = 4096\nlabel_smoothing = 0.01\nlearning_rate = 0.001\n\nopt_th = 0.500\n\nmodels = []\n\nif TRAINING:\n    date = train['date'].unique()\n\n    kf = KFold(n_splits=5, random_state=SEED, shuffle=False)\n    for k, (train_date, valid_date) in enumerate(kf.split(date)):\n        train_date, valid_date = date[train_date], date[valid_date]\n        \n        index_train, index_valid = train['date'].isin(train_date), train['date'].isin(valid_date)\n    \n        X_train, X_valid = train.loc[index_train,features].values, train.loc[index_valid,features].values\n        y_train, y_valid = train.loc[index_train,multi_target].values.astype('float16'), train.loc[index_valid,multi_target].values.astype('float16')\n        \n        print('Fold_{}'.format(k))\n        print('=============================================================================')\n        print('X_train:',X_train.shape)\n        print('y_train:',y_train.shape)\n        print('X_valid:',X_valid.shape)\n        print('y_valid:',y_valid.shape)\n        \n        #class_weights = {0: 1.,1: 1.1}\n        nn_model = create_mlp(len(features), len(multi_target), label_smoothing, learning_rate)\n\n        nn_model.fit(X_train,\n                     y_train,\n                     #class_weight=class_weights,\n                     epochs=epochs,\n                     validation_data=(X_valid,y_valid),\n                     batch_size=batch_size,\n                     callbacks=[EarlyStopping('val_loss',patience=2,restore_best_weights=True)]\n                    )\n        # restore model\n        models.append(nn_model)\n        \n        # save model\n        nn_model.save_weights('./model_fold{}.hdf5'.format(k))\n        \n        # validation\n        Valid = train[index_valid]\n\n        predictions = np.zeros(X_valid.shape[0])\n        predictions = np.median(nn_model(X_valid, training = False).numpy(),axis=1)\n\n        Valid.action = np.where(predictions >= opt_th, 1, 0).astype('int8')\n        print(\"Valid Jane Utility: {:.2f}\".format(jane_utility(train[index_valid], action_column=\"action\")))\n        print(\"Pred  Jane Utility: {:.2f}\".format(jane_utility(Valid, action_column=\"action\")))\n        print('accuracy:{:.3f}'.format(accuracy_score(train[index_valid].action.values, Valid.action.values)))\n        print('precision:{:.3f}'.format(precision_score(train[index_valid].action.values, Valid.action.values)))\n        print('recall:{:.3f}'.format(recall_score(train[index_valid].action.values, Valid.action.values)))\n    \nelse:\n    nn_model.load_weights(save_path+'model_fold{}.hdf5'.format(k))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sample_test"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading csv\nprint('Reading test.csv')\n\ntest = dt.fread(folder_path + 'example_test.csv').to_pandas()\n\nprint('Finish Reading')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.fillna(train.mean(),inplace=True)\ntest = test.loc[:, features].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = np.zeros(test.shape[0])\npredictions = np.median(np.mean([model(test, training = False).numpy() for model in models],axis=0),axis=1)\n\n\n\nsns.distplot(predictions,\n             kde_kws={'label': 'kde','color':'k'},\n             fit=norm,\n             fit_kws={'label': 'norm','color':'red'},\n             rug=False\n            )\n\nplt.legend()\nplt.xlim(0.2,0.8)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"th = 0.500\npd.DataFrame(np.where(predictions >= opt_th, 1, 0).astype(int)).value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Inference\nmodels = models[0]\n\nif Inf:\n\n    env = janestreet.make_env()\n\n    for (test_df, pred_df) in tqdm(env.iter_test()):\n        \n        if test_df['weight'].item() > 0:\n            x_tt = test_df.loc[:, features].values\n        \n            if np.isnan(x_tt[:, 1:].sum()):\n                x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n        \n            pred = np.median(np.mean([model(x_tt, training = False).numpy() for model in models],axis=0),axis=1)\n            pred_df.action = np.where(pred >= opt_th, 1, 0).astype(int)\n        \n        env.predict(pred_df)\nelse:\n    print('Traning Done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if Inf:\n    # display submission.csv\n    submission = pd.read_csv('./submission.csv')\n    print(submission.action.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if Inf:\n    sns.countplot(submission.action);","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}